name: demo
on: 
  workflow_dispatch:  # Manual trigger for testing
    inputs:
      instance-type:
        description: 'EC2 instance type to use'
        required: false
        default: 'c5.9xlarge,c7i.12xlarge,m7i.12xlarge,c6i.16xlarge'
        type: string
      pidstat-period:
        description: 'Collection frequency for pidstat in seconds'
        required: false
        default: '1'
        type: string
  push:
    branches:
      - main
    paths:
      - deploy/opentelemetry-demo/values.yaml
      - .github/workflows/demo.yaml

permissions:
  id-token: write # Required for requesting the JWT
  contents: read
  actions: write

jobs:
  setup-runner:
    name: Start EC2 runner
    runs-on: ubuntu-latest
    outputs:
      runner-label: ${{ steps.start-runner.outputs.runner-label }}
      ec2-instance-id: ${{ steps.start-runner.outputs.ec2-instance-id }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Start AWS Runner
        id: start-runner
        uses: ./.github/actions/aws-runner
        with:
          github-token: ${{ secrets.REPO_ADMIN_TOKEN }}
          aws-role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ secrets.AWS_REGION }}
          subnet-id: ${{ secrets.AWS_SUBNET_ID }}
          security-group-id: ${{ secrets.AWS_SECURITY_GROUP_ID }}
          iam-role-name: github-actions-runner          
          instance-type: ${{ inputs.instance-type || 'c5.9xlarge,c7i.12xlarge,m7i.12xlarge,c6i.16xlarge' }}
          aws-image-id: 'ami-0884d2865dbe9de4b'  # Ubuntu 22.04 LTS in us-east-2
          volume-size: '40'

  run-workload:
    needs: [setup-runner]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 30
    outputs:
      uuid-prefix: ${{ steps.generate-uuid.outputs.uuid }}
    env:
      RELEASE_NAME: otel-demo
      COLLECTOR_RELEASE_NAME: collector
      S3_BUCKET: "unvariance-collector-test-irsa"
      AWS_REGION: ${{ secrets.AWS_REGION }}
      HOME: /root
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      PIDSTAT_PERIOD: ${{ inputs.pidstat-period || '1' }}
    steps:
      - name: Create HOME directory
        run: |
          mkdir -p $HOME

      - name: Install K3s Cluster
        run: |
          # Installs K3s (a lightweight Kubernetes distribution) on the system
          curl -sfL https://get.k3s.io | sh

      - name: Status of K3s Installation
        run: |
          systemctl status k3s  
      
      - name: Wait for Kubernetes API
        run: |
          echo "Waiting for Kubernetes API..."
          until kubectl get nodes &>/dev/null; do
            sleep 1
            echo "Still waiting..."
          done
          echo "Kubernetes API is available!"

      - name: Wait for nodes
        run: |
          echo "Waiting for at least one node to be registered..."
          until [ $(kubectl get nodes --no-headers | wc -l) -gt 0 ]; do
            sleep 1
            echo "Still waiting for node registration..."
          done
          echo "Node(s) registered, waiting for Ready status..."
          kubectl wait --for=condition=Ready nodes --all --timeout=300s      

      - name: Get Default objects in kube-system
        run: | 
          kubectl get all -n kube-system

      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

      - name: Checkout repository
        uses: actions/checkout@v4

      # Install AWS CLI before deploying the collector
      - name: Install awscli
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          python3 -m zipfile -e awscliv2.zip .
          chmod u+x ./aws/install
          sudo ./aws/install
          echo ls: `ls -l /usr/local/bin/aws` || true
          chmod +x /usr/local/bin/aws || true
          echo version: `/usr/local/bin/aws --version` || true

      # Generate UUID and deploy collector before OpenTelemetry
      - name: Generate UUID Prefix
        id: generate-uuid
        run: |
          UUID=$(python3 -c "import uuid; print(uuid.uuid4())")
          echo "Using UUID prefix: $UUID"
          echo "uuid=$UUID" >> $GITHUB_OUTPUT

      - name: Add Unvariance Helm Repository
        run: |
          helm repo add unvariance https://unvariance.github.io/collector/charts
          helm repo update

      - name: Deploy Collector Helm Chart
        run: |
          UUID_PREFIX="${{ steps.generate-uuid.outputs.uuid }}-"
          
          # Install the helm chart using --set for values
          helm install ${COLLECTOR_RELEASE_NAME} unvariance/collector \
            --set collector.image.repository="ghcr.io/yonch/memory-collector/collector" \
            --set collector.verbose=true \
            --set collector.parquetFileSize=1000000 \
            --set storage.type=s3 \
            --set storage.prefix="${UUID_PREFIX}" \
            --set storage.s3.bucket="${S3_BUCKET}" \
            --set storage.s3.region="${AWS_REGION}" \
            --set storage.s3.auth.method=iam \
            --set resources.limits.cpu=1000m \
            --set resources.requests.cpu=1000m \
            --wait

      - name: Wait for Collector Pods to be Ready
        run: |
          kubectl wait --for=condition=Ready pods --timeout=60s -l app.kubernetes.io/name=collector || WAIT_STATUS=$?
          echo "Wait exit status: $WAIT_STATUS"
          
          # Always describe pods, regardless of wait result
          echo "Describing collector pods:"
          kubectl describe pods -l app.kubernetes.io/name=collector
          
          # Only fail the job if wait failed
          if [ -n "$WAIT_STATUS" ] && [ "$WAIT_STATUS" -ne 0 ]; then
            echo "ERROR: Collector pods are not ready after timeout"
            exit 1
          fi

      - name: Show Collector Pod Status
        run: |
          kubectl get pods -l app.kubernetes.io/name=collector
          
      - name: Send SIGUSR1 to collector to rotate files
        run: |
          # Get the collector pod name
          COLLECTOR_POD=$(kubectl get pods -l app.kubernetes.io/name=collector -o name | cut -d/ -f2)
          echo "Collector Pod: $COLLECTOR_POD"
          
          # Send SIGUSR1 to the collector to rotate files
          kubectl exec $COLLECTOR_POD -c collector -- sh -c 'kill -USR1 1' || true
          
          # Wait a moment for the rotation to complete
          timeout 20s kubectl logs -f --tail=-1 $COLLECTOR_POD -c collector | grep -m 1 "Closed" || true
          
          # Check logs for file closure
          kubectl logs $COLLECTOR_POD -c collector | grep -i "Closed"

      - name: Verify file was uploaded to S3
        run: |
          # Get UUID prefix from the output
          UUID_PREFIX="${{ steps.generate-uuid.outputs.uuid }}-"
          
          # List files with the UUID prefix
          echo "Checking for files with prefix ${UUID_PREFIX} in S3 bucket ${S3_BUCKET}"
          S3_FILES=$(aws s3 ls "s3://${S3_BUCKET}/${UUID_PREFIX}" --recursive || echo "")
          
          if [ -z "$S3_FILES" ]; then
            echo "No files found with prefix ${UUID_PREFIX} in bucket ${S3_BUCKET}"
            exit 1
          else
            echo "Found files with prefix ${UUID_PREFIX}:"
            echo "$S3_FILES"
            echo "File rotation successful!"
          fi

      # Now deploy OpenTelemetry after the collector is running
      - name: Add OpenTelemetry Helm Repository
        run: |
          helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
          helm repo update

      - name: Install OpenTelemetry Demo
        run: |
          helm install $RELEASE_NAME open-telemetry/opentelemetry-demo -f deploy/opentelemetry-demo/values.yaml

      # Configure apt while OpenTelemetry pods are coming up
      - name: Disable IPv6
        run: |
          # Disable IPv6 via sysctl
          sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1
          sudo sysctl -w net.ipv6.conf.default.disable_ipv6=1
          sudo sysctl -w net.ipv6.conf.lo.disable_ipv6=1

          # Force apt to use IPv4
          echo 'Acquire::ForceIPv4 "true";' | sudo tee /etc/apt/apt.conf.d/99force-ipv4

      - name: Configure apt to use HTTPS
        run: |
          # Update all archive URLs to use HTTPS
          sudo sed -i 's/http:/https:/g' /etc/apt/sources.list

          # Install apt-transport-https (might fail initially, hence the || true)
          sudo apt-get update || true
          sudo apt-get install -y apt-transport-https ca-certificates

          # Update again with HTTPS now configure
          sudo apt-get update

      - name: Install sysstat
        run: |
          sudo apt-get install -y sysstat

      # Wait for kube-system pods after microservices installation starts
      - name: Wait for kube-system pods
        run: |
          echo "Waiting for at least one kube-system pod to be registered..."
          until [ $(kubectl get pods --namespace kube-system --no-headers | wc -l) -gt 0 ]; do
            sleep 1
            echo "Still waiting for kube-system pod registration..."
          done
          echo "Kube-system pod(s) registered, waiting for Ready status..."
          kubectl wait --namespace kube-system --for=condition=Ready pods --all --timeout=300s

      - name: Wait for all Pods to be Ready
        run: |
          if ! kubectl wait --for=condition=Ready pods --all --timeout=150s; then
            echo "Error: Not all pods reached Ready state within timeout"
            kubectl get pods
            echo "--------------------------------"
            kubectl describe pods
            exit 1
          fi

      - name: Print Events
        run: | 
          kubectl get events

      - name: Disk Space Size
        run: | 
          df -h

      - name: Print Pod Status
        run: | 
          kubectl get pods -n default
      
      - name: Describe Frontend Deployment
        run: |
          kubectl describe deployment frontend

      - name: Describe Load Generator Deployment
        run: |
          kubectl describe deployment load-generator

      - name: Start PIDs stat collection
        id: start-pidstat
        run: |
          # Create directory for metrics
          mkdir -p /tmp/system_metrics
          
          # Set file paths
          CPU_METRICS_FILE="/tmp/system_metrics/cpu_metrics.csv"
          MEMORY_METRICS_FILE="/tmp/system_metrics/memory_metrics.csv"
          
          # Start CPU metrics collection in background
          pidstat -H -u -p ALL ${PIDSTAT_PERIOD} | awk '{gsub(/^ +| +$/,""); gsub(/ +/,";"); print}' > ${CPU_METRICS_FILE} 2>&1 &
          CPU_PIDSTAT_PID=$!
          
          # Start memory metrics collection in background
          pidstat -H -r -p ALL ${PIDSTAT_PERIOD} | awk '{gsub(/^ +| +$/,""); gsub(/ +/,";"); print}' > ${MEMORY_METRICS_FILE} 2>&1 &
          MEMORY_PIDSTAT_PID=$!
          
          # Verify processes are running
          ps -p $CPU_PIDSTAT_PID
          ps -p $MEMORY_PIDSTAT_PID
          
          # Set environment variables for later use
          echo "CPU_METRICS_FILE=${CPU_METRICS_FILE}" >> $GITHUB_ENV
          echo "MEMORY_METRICS_FILE=${MEMORY_METRICS_FILE}" >> $GITHUB_ENV
          echo "CPU_PIDSTAT_PID=${CPU_PIDSTAT_PID}" >> $GITHUB_ENV
          echo "MEMORY_PIDSTAT_PID=${MEMORY_PIDSTAT_PID}" >> $GITHUB_ENV
          
          echo "Started PIDs stat collection:"
          echo "CPU metrics PID: ${CPU_PIDSTAT_PID}"
          echo "Memory metrics PID: ${MEMORY_PIDSTAT_PID}"
          echo "CPU metrics file: ${CPU_METRICS_FILE}"
          echo "Memory metrics file: ${MEMORY_METRICS_FILE}"

      - name: Display logs while collector runs
        run: |
          timeout 20s kubectl logs -f --tail=-1 -l app.kubernetes.io/name=collector || true

      - name: Stop PIDs stat collection
        run: |
          echo "Stopping PIDs stat collection processes"
          kill -TERM $CPU_PIDSTAT_PID || true
          kill -TERM $MEMORY_PIDSTAT_PID || true
          
          # Wait a moment to ensure files are fully written
          sleep 2
          
          # Check if files exist and have content
          echo "CPU metrics file size: $(stat -c %s $CPU_METRICS_FILE || echo 'file not found')"
          echo "Memory metrics file size: $(stat -c %s $MEMORY_METRICS_FILE || echo 'file not found')"
          
          # Create directory for system metrics
          mkdir -p system_metrics
          
          # Copy the files to the upload directory
          cp $CPU_METRICS_FILE system_metrics/cpu_metrics.csv || echo "Failed to copy CPU metrics file"
          cp $MEMORY_METRICS_FILE system_metrics/memory_metrics.csv || echo "Failed to copy memory metrics file"
          
          # List the files
          echo "System metrics files:"
          ls -la system_metrics/

      - name: Print events
        run: |
          kubectl get events

      - name: Debug load-generator pod
        run: |
          LOADGEN_POD=$(kubectl get pods -l app.kubernetes.io/component=load-generator -o name | cut -d/ -f2)
          echo "Load Generator Pod: $LOADGEN_POD"

          # Get the logs of the previous load-generator pod
          echo "--------------------------------"
          echo "Getting PREVIOUS logs of load-generator pod"
          kubectl logs $LOADGEN_POD --previous || true

          # Get the logs of the load-generator pod
          echo "--------------------------------"
          echo "Getting logs of load-generator pod"
          kubectl logs $LOADGEN_POD || true

          # Describe the load-generator pod
          echo "--------------------------------"
          echo "Describing load-generator pod"
          kubectl describe pod $LOADGEN_POD || true

      - name: Extract Locust CSV files
        run: |
          # Create directory for the results
          mkdir -p locust_results
          
          # Get the load-generator pod name
          LOADGEN_POD=$(kubectl get pods -l app.kubernetes.io/component=load-generator -o name | cut -d/ -f2)
          echo "Load Generator Pod: $LOADGEN_POD"

          # Copy CSV files from the pod
          echo "Copying CSV files from pod..."
          kubectl cp $LOADGEN_POD:/tmp/locust_results/ ./locust_results/
          
          # List the extracted files
          echo "Extracted CSV files:"
          ls -la locust_results/
          
      - name: Upload Locust Results as Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: locust-csv-results
          path: locust_results/
          retention-days: 28

      - name: Upload System Metrics as Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: system-metrics-results
          path: system_metrics/
          retention-days: 28

      - name: Uninstall Collector Helm Chart
        run: |
          # see the logs while uninstalling
          kubectl logs -f --tail=-1 -l app.kubernetes.io/name=collector || true &
          LOGS_PID=$!

          # trigger a rotation by sending a SIGUSR1 to the collector pod
          # get the collector pod name
          COLLECTOR_POD=$(kubectl get pods -l app.kubernetes.io/name=collector -o name | cut -d/ -f2)
          echo "Collector Pod: $COLLECTOR_POD"
          kubectl exec $COLLECTOR_POD -c collector -- sh -c 'kill -USR1 1' || true

          # wait for a few seconds to see if there is a flush
          sleep 10

          # wait for the pods to be deleted, in the background
          kubectl wait --for=delete pods -l app.kubernetes.io/name=collector --timeout=45s || true &
          WAIT_PID=$!

          # uninstall the chart
          helm uninstall ${COLLECTOR_RELEASE_NAME}

          # wait for the pods to be deleted
          wait $WAIT_PID || true

          # kill the logs
          kill -TERM $LOGS_PID || true

      - name: Collector logs
        run: |
          kubectl logs -l app.kubernetes.io/name=collector || true

      - name: Verify multiple files were uploaded to S3
        run: |
          # Get UUID prefix from the output
          UUID_PREFIX="${{ steps.generate-uuid.outputs.uuid }}-"
          
          # List files with the UUID prefix
          echo "Checking for files with prefix ${UUID_PREFIX} in S3 bucket ${S3_BUCKET}"
          S3_FILES=$(aws s3 ls "s3://${S3_BUCKET}/${UUID_PREFIX}" --recursive || echo "")
          
          if [ -z "$S3_FILES" ]; then
            echo "No files found with prefix ${UUID_PREFIX} in bucket ${S3_BUCKET}"
            exit 1
          else
            echo "Found files with prefix ${UUID_PREFIX}:"
            echo "$S3_FILES"
            
            # Count the number of files
            FILE_COUNT=$(echo "$S3_FILES" | wc -l)
            echo "Number of collector files: ${FILE_COUNT}"
            
            if [ "$FILE_COUNT" -lt 2 ]; then
              echo "ERROR: Expected at least 2 files, but found only ${FILE_COUNT}"
              exit 1
            fi
            
            # Get the last file path
            LAST_FILE=$(echo "$S3_FILES" | tail -n 1 | awk '{print $4}')
            
            # Download the last file for validation
            aws s3 cp "s3://${S3_BUCKET}/${LAST_FILE}" /tmp/collector-parquet.parquet
            
            # Check file size
            FILE_SIZE=$(stat -c %s /tmp/collector-parquet.parquet)
            echo "Downloaded last collector file size: ${FILE_SIZE} bytes"
          fi

      - name: Upload Collector Results as Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: collector-parquet-results
          path: /tmp/collector-parquet.parquet
          retention-days: 28

  stop-runner:
    name: Stop EC2 runner
    needs: [setup-runner, run-workload]
    runs-on: ubuntu-latest
    if: always()  # Run even if previous jobs fail
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Stop AWS Runner
        uses: ./.github/actions/aws-runner/cleanup
        with:
          runner-label: ${{ needs.setup-runner.outputs.runner-label }}
          ec2-instance-id: ${{ needs.setup-runner.outputs.ec2-instance-id }}
          github-token: ${{ secrets.REPO_ADMIN_TOKEN }}
          aws-role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ secrets.AWS_REGION }} 