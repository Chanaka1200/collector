name: demo
on: 
  workflow_dispatch:  # Manual trigger for testing
    inputs:
      instance-type:
        description: 'EC2 instance type to use'
        required: false
        default: 'c5.9xlarge'
        type: string
  push:
    branches:
      - main
    paths:
      - deploy/opentelemetry-demo/values.yaml
      - .github/workflows/demo.yaml

permissions:
  id-token: write # Required for requesting the JWT
  contents: read
  actions: write

jobs:
  setup-runner:
    name: Start EC2 runner
    runs-on: ubuntu-latest
    outputs:
      runner-label: ${{ steps.start-runner.outputs.runner-label }}
      ec2-instance-id: ${{ steps.start-runner.outputs.ec2-instance-id }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Start AWS Runner
        id: start-runner
        uses: ./.github/actions/aws-runner
        with:
          github-token: ${{ secrets.REPO_ADMIN_TOKEN }}
          aws-role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ secrets.AWS_REGION }}
          subnet-id: ${{ secrets.AWS_SUBNET_ID }}
          security-group-id: ${{ secrets.AWS_SECURITY_GROUP_ID }}
          instance-type: ${{ inputs.instance-type || 'c5.9xlarge' }}
          aws-image-id: 'ami-0884d2865dbe9de4b'  # Ubuntu 22.04 LTS in us-east-2
          volume-size: '40'

  k3-deployment:
    needs: [setup-runner]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 10
    env:
      HOME: /root
    steps:
      - name: Create HOME directory
        run: |
          mkdir -p $HOME

      - name: Install K3 Cluster
        run: |
          # Installs K3s (a lightweight Kubernetes distribution) on the system
          curl -sfL https://get.k3s.io | sh

      - name: Status of K3s Installation
        run: |
          systemctl status k3s  
      
      - name: Wait for Kubernetes API
        run: |
          echo "Waiting for Kubernetes API..."
          until kubectl get nodes &>/dev/null; do
            sleep 1
            echo "Still waiting..."
          done
          echo "Kubernetes API is available!"

      - name: Wait for nodes
        run: |
          echo "Waiting for at least one node to be registered..."
          until [ $(kubectl get nodes --no-headers | wc -l) -gt 0 ]; do
            sleep 1
            echo "Still waiting for node registration..."
          done
          echo "Node(s) registered, waiting for Ready status..."
          kubectl wait --for=condition=Ready nodes --all --timeout=300s      

      - name: Wait for kube-system pods
        run: |
          echo "Waiting for at least one kube-system pod to be registered..."
          until [ $(kubectl get pods --namespace kube-system --no-headers | wc -l) -gt 0 ]; do
            sleep 1
            echo "Still waiting for kube-system pod registration..."
          done
          echo "Kube-system pod(s) registered, waiting for Ready status..."
          kubectl wait --namespace kube-system --for=condition=Ready pods --all --timeout=300s

      - name: Get Default objects in kube-system
        run: | 
          kubectl get all -n kube-system

  microservice-deployment:
    needs: [setup-runner,k3-deployment]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 15
    env:
      USERS: 3000000
      RATE: 10000
      RELEASE_NAME: otel-demo
      HOME: /root
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

      - name: Add OpenTelemetry Helm Repository
        run: |
          helm repo add open-telemetry https://open-telemetry.github.io/opentelemetry-helm-charts
          helm repo update

      - name: Install OpenTelemetry Demo
        run: |
          helm install $RELEASE_NAME open-telemetry/opentelemetry-demo -f deploy/opentelemetry-demo/values.yaml

      - name: Wait for all Pods to be Ready
        run: |
          if ! kubectl wait --for=condition=Ready pods --all --timeout=150s; then
            echo "Error: Not all pods reached Ready state within timeout"
            kubectl get pods
            echo "--------------------------------"
            kubectl describe pods
            exit 1
          fi

      - name: Print Events
        run: | 
          kubectl get events

      - name: Disk Space Size
        run: | 
          df -h

      - name: Print Pod Status
        run: | 
          kubectl get pods -n default
      
      - name: Describe Frontend Deployment
        run: |
          kubectl describe deployment frontend

      - name: Describe Load Generator Deployment
        run: |
          kubectl describe deployment load-generator
          
      # - name: Accessible URL
      #   run: |
      #     # Get accessible URL 
      #     echo "http://$(kubectl get svc frontendproxy --no-headers -o wide | awk '{print $4}')"

  collector-deployment:
    needs: [setup-runner, microservice-deployment]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 10
    outputs:
      uuid-prefix: ${{ steps.generate-uuid.outputs.uuid }}
    env:
      RELEASE_NAME: collector
      S3_BUCKET: "unvariance-collector-test-irsa"
      AWS_REGION: ${{ secrets.AWS_REGION }}
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      HOME: /root
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install Helm
        run: |
          curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

      - name: Generate UUID Prefix
        id: generate-uuid
        run: |
          UUID=$(python3 -c "import uuid; print(uuid.uuid4())")
          echo "Using UUID prefix: $UUID"
          echo "uuid=$UUID" >> $GITHUB_OUTPUT

      - name: Add Unvariance Helm Repository
        run: |
          helm repo add unvariance https://unvariance.github.io/collector/charts
          helm repo update

      - name: Deploy Collector Helm Chart
        run: |
          UUID_PREFIX="${{ steps.generate-uuid.outputs.uuid }}-"
          
          # Install the helm chart using --set for values
          helm install ${RELEASE_NAME} unvariance/collector \
            --set collector.verbose=true \
            --set storage.type=s3 \
            --set storage.prefix="${UUID_PREFIX}" \
            --set storage.s3.bucket="${S3_BUCKET}" \
            --set storage.s3.region="${AWS_REGION}" \
            --set storage.s3.auth.method=iam

      - name: Wait for Collector Pods to be Ready
        run: |
          kubectl wait --for=condition=Ready pods --timeout=60s -l app.kubernetes.io/name=collector || WAIT_STATUS=$?
          echo "Wait exit status: $WAIT_STATUS"
          
          # Always describe pods, regardless of wait result
          echo "Describing collector pods:"
          kubectl describe pods -l app.kubernetes.io/name=collector
          
          # Only fail the job if wait failed
          if [ -n "$WAIT_STATUS" ] && [ "$WAIT_STATUS" -ne 0 ]; then
            echo "ERROR: Collector pods are not ready after timeout"
            exit 1
          fi

      - name: Show Pod Status
        run: |
          kubectl get pods
          kubectl describe pods -l app.kubernetes.io/name=collector

      - name: Display logs while collector runs
        run: |
          timeout 20s kubectl logs -f -l app.kubernetes.io/name=locust || true

      - name: Uninstall Collector Helm Chart
        run: |
          helm uninstall ${RELEASE_NAME} --wait --timeout=60s

      - name: Collector logs
        run: |
          kubectl logs -l app.kubernetes.io/name=collector || true

  collect-data:
    needs: [setup-runner, microservice-deployment, collector-deployment]
    runs-on: ${{ needs.setup-runner.outputs.runner-label }}
    timeout-minutes: 10
    env:
      HOME: /root
      KUBECONFIG: /etc/rancher/k3s/k3s.yaml
      S3_BUCKET: "unvariance-collector-test-irsa"
      AWS_REGION: ${{ secrets.AWS_REGION }}
      UUID_PREFIX: ${{ needs.collector-deployment.outputs.uuid-prefix }}
    steps:

      - name: Print events
        run: |
          kubectl get events

      - name: Debug load-generator pod
        run: |
          LOADGEN_POD=$(kubectl get pods -l app.kubernetes.io/component=load-generator -o name | cut -d/ -f2)
          echo "Load Generator Pod: $LOADGEN_POD"

          # Get the logs of the previous load-generator pod
          echo "--------------------------------"
          echo "Getting PREVIOUS logs of load-generator pod"
          kubectl logs $LOADGEN_POD --previous || true

          # Get the logs of the load-generator pod
          echo "--------------------------------"
          echo "Getting logs of load-generator pod"
          kubectl logs $LOADGEN_POD || true

          # Describe the load-generator pod
          echo "--------------------------------"
          echo "Describing load-generator pod"
          kubectl describe pod $LOADGEN_POD || true

      - name: Install awscli
        run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          python3 -m zipfile -e awscliv2.zip .
          chmod u+x ./aws/install
          sudo ./aws/install
          echo ls: `ls -l /usr/local/bin/aws` || true
          chmod +x /usr/local/bin/aws || true
          echo version: `/usr/local/bin/aws --version` || true

      - name: Check for Collector Files in S3
        run: |
          echo "Checking for files with prefix ${UUID_PREFIX} in S3 bucket ${S3_BUCKET}"
          
          # List files with the UUID prefix
          S3_FILES=$(aws s3 ls "s3://${S3_BUCKET}/${UUID_PREFIX}" --recursive || echo "")
          
          if [ -z "$S3_FILES" ]; then
            echo "No files found with prefix ${UUID_PREFIX} in bucket ${S3_BUCKET}"
            exit 1
          else
            echo "Found files with prefix ${UUID_PREFIX}:"
            echo "$S3_FILES"
            
            # Get the first file path
            FIRST_FILE=$(echo "$S3_FILES" | head -n 1 | awk '{print $4}')
            
            # Download the file for validation
            aws s3 cp "s3://${S3_BUCKET}/${FIRST_FILE}" /tmp/collector-parquet.parquet
            
            # Check file size
            FILE_SIZE=$(stat -c %s /tmp/collector-parquet.parquet)
            echo "Downloaded collector file size: ${FILE_SIZE} bytes"
          fi

      - name: Extract Locust CSV files
        run: |
          # Create directory for the results
          mkdir -p locust_results
          
          # Get the load-generator pod name
          LOADGEN_POD=$(kubectl get pods -l app.kubernetes.io/component=load-generator -o name | cut -d/ -f2)
          echo "Load Generator Pod: $LOADGEN_POD"

          # Copy CSV files from the pod
          echo "Copying CSV files from pod..."
          kubectl cp $LOADGEN_POD:/tmp/locust_results/ ./locust_results/
          
          # List the extracted files
          echo "Extracted CSV files:"
          ls -la locust_results/
          
      - name: Upload Locust Results as Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: locust-csv-results
          path: locust_results/
          retention-days: 28

      - name: Upload Collector Results as Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: collector-parquet-results
          path: /tmp/collector-parquet.parquet
          retention-days: 28

  stop-runner:
    name: Stop EC2 runner
    needs: [setup-runner, k3-deployment, microservice-deployment, collector-deployment, collect-data]
    runs-on: ubuntu-latest
    if: always()  # Run even if previous jobs fail
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        
      - name: Stop AWS Runner
        uses: ./.github/actions/aws-runner/cleanup
        with:
          runner-label: ${{ needs.setup-runner.outputs.runner-label }}
          ec2-instance-id: ${{ needs.setup-runner.outputs.ec2-instance-id }}
          github-token: ${{ secrets.REPO_ADMIN_TOKEN }}
          aws-role-arn: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ secrets.AWS_REGION }} 